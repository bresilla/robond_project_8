{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Lab Notebook\n",
    "\n",
    "In this notebook, you will learn the basics applying the Q-Learning algorithm to simple environments in OpenAI Gym.  By the end of the notebook, you will have a working Q-Learning agent that can balance a CartPole as well as solve other simple control problems in alternate environments.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. <a href=#rl>Basic concept of reinforcement learning (RL)</a>\n",
    "- <a href=#cp>CartPole environment from OpenAI Gym</a>\n",
    "- <a href=#random>CartPole with a random agent</a>\n",
    "- <a href=#qlcode>Q-Learning Implementation in Python</a>\n",
    "- <a href=#qlagent>CartPole with a Q-Learning agent</a>\n",
    "- <a href=#mtn>MountainCar environment from OpenAI Gym</a>\n",
    "\n",
    "## Requirements\n",
    "* OpenAI Gym[classic_control] 0.7.4 or higher \n",
    "* Python 3.5 or higher\n",
    "* CUDA8.0 enabled GPU\n",
    "\n",
    "Portions of this notebook have been borrowed from the Nvidia Deep Learning Institute (DLI) hands-on labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement learning <a name='rl' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setting\n",
    "\n",
    "Unlike standard supervised learning in machine learning, reinforcement learning is about sequential decision making of an agent, which takes actions to maximize cumulative reward, by interacting with environments (problems).\n",
    "\n",
    "<img src=\"image/rl.png\" width=\"200\">\n",
    "\n",
    "After observing the current state $s_t$ and reward $r_t$, agent will decide which action to take $a_t$.\n",
    "\n",
    "For clarity, here we assume that actions are discrete, and reward is a scalar value. Multi-dimensional state $s_t$ can be either discrete or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value function approximation\n",
    "\n",
    "There are multiple strategies in RL on how to represent the policy, the behavior mechanism inside the agent and how to optimize it.\n",
    "\n",
    "In most of this hands-on session, we just consider action-value function approximation, in which the expected cumulative reward for all future steps is represented as a function $Q(s, a)$ for pairs of state $s$ and following action $a$.\n",
    "\n",
    "By learning a good approximation of optimal action-value function $Q(s, a)$, by taking action $a_t$ which maximizes the learned Q-value $Q(s_t, a_t)$ given $s_t$ at each time step $t$, the optimal policy should be realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example - CartPole <a name='cp' />\n",
    "\n",
    "In this section, we introduce a control problem named CartPole and describe the way it is handled as a RL problem.  Before looking closer at the CartPole environment, we'll import the packages we need and define a visualization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named pip\r\n"
     ]
    }
   ],
   "source": [
    "# install packages in Udacity Workspaces\n",
    "# xvfb-run -s \"-screen 0 1400x900x24\" bash && export DISPLAY=:0\n",
    "!python -m pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment for Udacity server using xvfb\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization helpers\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set logger level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Environment\n",
    "\n",
    "As an example of classical RL problem, we use CartPole-v0 from [OpenAI Gym](https://gym.openai.com/), which contains two-dimensional physics simulator of a black cart and a yellow pole. It is a kind of inverted pendulum.\n",
    "\n",
    "<img src=\"image/cartpole.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following overview of CartPole-v0 is taken from [Gym's Wiki page](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "#### Description\n",
    "By an un-actuated joint, a pole is attached to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "#### Observation\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "#### Conditions for episode termination \n",
    "1. Pole Angle is more than ±20.9°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "#### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "#### Starting state\n",
    "All observations are assigned a uniform random value between ±0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Gym's environment works\n",
    "\n",
    "Gym's environment (problem) has a unified interface with the following three steps.\n",
    "\n",
    "#### 1. Create environment specified by name\n",
    "```python\n",
    "env = gym.make('ENV_NAME')\n",
    "```\n",
    "#### 2. Initialize environment\n",
    "\n",
    "```python\n",
    "env.reset()\n",
    "```\n",
    "#### 3. Take action and observe reward & next state\n",
    "```python\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "```\n",
    "where\n",
    "  - obs: a next observation\n",
    "  - reaward: a scalar reward\n",
    "  - is_finished: a boolean value indicating whether the current state is terminal or not\n",
    "  - info: additional information\n",
    "\n",
    "By interacting the environment, a reinforcement learning agent learns how to optimize its strategy for maximizing cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: How the environment works\n",
    "\n",
    "Create the CartPole environment and observe the initial state and the result of an action.  In this case a random action has been selected by sampling the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "initial observation: [-0.01457592  0.04801101  0.01353517 -0.00053875]\n",
      "random action: 0\n",
      "next observation: [-0.0136157  -0.14730241  0.01352439  0.29638377]\n",
      "reward: 1.0\n",
      "is_finished: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "# Create  environment of CartPole-v0\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "observation = env.reset()\n",
    "# env.render(mode='rgb_array', close=True)\n",
    "print('initial observation:', observation)\n",
    "\n",
    "action = env.action_space.sample() # Select random action\n",
    "print('random action:', action)\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "print('next observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('is_finished:', is_finished)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CartPole with the Random agent <a name='random' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from gym examples - a model for our agent\n",
    "class RandAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done=None, mode=None):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def init_episode(self, observation):\n",
    "        # provided for compatibility with general learner\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Learner for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This learner provides a general interface for agents and environments \n",
    "# optionally visualize within a Jupyter Notebook when visualize_plt is True\n",
    "def learner(agent=None, env_id='CartPole-v0', episodes=100, max_length = 100, init_reward=0, \n",
    "            ignore_done=False, visualize_plt=True, mode=None):\n",
    "    # load the environment \n",
    "    env = gym.make(env_id)\n",
    "    # set the agent to random if none provided\n",
    "    if agent is None:\n",
    "        agent = RandAgent(env.action_space)\n",
    "\n",
    "    # each episode runs until it is observed as finished, or exceeds max_length in time steps\n",
    "    episode_count = episodes\n",
    "    done = False\n",
    "    n_steps = np.zeros((episode_count,))\n",
    "\n",
    "    # run the episodes - use tqdm to track in the notebook\n",
    "    for i in tqdm(range(episode_count), disable=visualize_plt):\n",
    "        \n",
    "        # Initialize environment for each episode\n",
    "        ob = env.reset()  \n",
    "        reward = init_reward\n",
    "        if visualize_plt:\n",
    "            img = plt.imshow(env.render(mode='rgb_array')) # only call this once, only for jupyter\n",
    "        \n",
    "        # initialize the agent\n",
    "        agent.init_episode(ob)\n",
    "        n_steps[i]=max_length\n",
    "        \n",
    "        # run the steps in each epsisode\n",
    "        for t in range(max_length):\n",
    "            # render the environment\n",
    "            if visualize_plt:\n",
    "                img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "                plt.axis('off')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                env.render()\n",
    "            \n",
    "            # get agent's action\n",
    "            action = agent.act(ob, reward, mode=mode)\n",
    "            # take the action and get reward and updated observation\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # terminate the steps if the problem is done\n",
    "            if done and not ignore_done:\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "            if done and ignore_done and env_id=='MountainCar-v0' and ob[0]>= 0.5:\n",
    "                # special case MountainCar\n",
    "                # if have achieved the goal, then quit but otherwise keep going\n",
    "                print(\"Episode {} done at step {}\".format(i,t))\n",
    "                print(\"Observations {}, Reward {}\".format(ob, reward))\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "    env.close()\n",
    "    return n_steps  # stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Run the experiment with the random agent\n",
    "\n",
    "Before introducing an RL agent, you can just repeat random actions to see how the environment changes over time. Here you repeat the episode for 10 times.  The number of steps before the cart fails are recorded for each episode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABH1JREFUeJzt28FtGlEUQFEmoonUkZSROpw2aMOuI2U4daQMsiGSBTiAsf1n7pyzRCD9BVw9zX9M+/1+A0DPl9EHAOBjCDxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRG1HH+DA32kBTk33fNgEDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUdvRB4DP9vvp58lr3x4eB5wEPpYJHiBK4AGiBB4gSuBhc/65PCydwANECTxAlMCzOlYiWQuBB4gSeFbp3BTvopUagQeIEniAKIEHiBJ4gCiBhxdctFIi8KyWfXjqBB4gSuDhiMc0VAg8QJTAA0QJPKvmopUygQeIEng4w0UrBQIPECXwAFECz+q5aKVK4AGiBB4gSuDhFTZpWDqBB4gSeIAogYeNTRqaBB4gSuDhP1y0smQCDxAl8ABRAg8HLlqpEXiAKIEHiBJ4uMAmDUsl8ABRAg8QJfDwgk0aSgQeIErg4QouWlkigYcjHtNQIfAAUQIPV/KYhqUReIAogQeIEng4w0UrBQIPECXwcAMXrSyJwANECTxAlMDDK1y0snQCDxAl8ABRAg83sknDUgg8QNR29AFglGmarnrf8+PDmz/7z36/v+n98B5M8ABRJni4wq8/x1P805BzwC1M8HDBadw3m93uecBJ4DYCDxAl8ABRAg8X7HbfT1778dUzeOZvmsn61iwOwbrcuup4j5n8zlieu76ks9ii+cwfGozgO85b3DsYzCLwphtGMMFT5xk8QJTAA0QJPECUwANECTxAlMADRAk8QNQs9uBhBLvp1JngAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGitqMPcDCNPgBAjQkeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4g6i/c+Ew2XU4e/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "max_length = 200\n",
    "steps = learner(episodes=num_episodes, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 10 episodes: 8.0\n",
      "Average step count in 10 episodes: 16.5\n",
      "Maximum step count in 10 episodes: 33.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Traditional RL = Q-learning <a name='qlcode' />\n",
    "\n",
    "Q-learning is one of the most popular approaches in reinforcement learning, to determine which action is optimal on each state.\n",
    "\n",
    "At each time step $t$, after taking an action $a_t$ and given reward $r_{t+1}$ and next state $s_{t+1}$, the old Q-value $Q(s_t, a_t)$ is updated by the following rule. \n",
    "\n",
    "<img src=\"image/q-learning.svg\" width=\"800\">\n",
    "\n",
    "It is not necessary to follow the details of math but the intuition is: the Q-value should be updated by taking into account the difference between the old value and the sum of the actual reward and the discounted estimated future value obtained by next action, by the factor of learning rate $\\alpha_t$.\n",
    "\n",
    "Q-learning is an iterative algorithm, so Q-values are used to take actions and then gradually improved over time.\n",
    "\n",
    "To make the good balance between exploration (random action) and exploitation (estimated best action), $\\epsilon$-greedy strategy is typically used with Q-learning in which where the agent take random actions with probability $\\epsilon$  and current best action otherwise.\n",
    "\n",
    "The simplest way to implement Q-Learning is to build a table, named Q-table, to store the Q-value for each discrete state and action combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Q-learning agent\n",
    "\n",
    "Here we define the QLearningAgent class, in which the observation space is discretized into q-tables of size 20,000 =  ((that of bins (9)+1) ^ numbers of observations (4)) * number of actions (2). Each cell represents current Q-value for each combination of discretized state and possible action.\n",
    "\n",
    "By calling the `act()` method with the current observation (and reward), the agent updates the Q-table and returns the next action.\n",
    "\n",
    "The hyper-parameters for training have been set at default values, but may be modified when called:\n",
    "\n",
    "**`learning_rate` = 0.2**\n",
    "* the new information replaces 20% of the old information at each step\n",
    "\n",
    "**`discount_factor` = 1.0** \n",
    "* the agent will strive for a long-term high reward because the value includes expected future rewards\n",
    "\n",
    "**`exploration_rate` = 0.5**\n",
    "* the agent will choose a random action, i.e. \"explore\", 50% of the time\n",
    "\n",
    "**`exploration_decay_rate` = 0.99**\n",
    "* the probability of exploration will be reduced by 1% at the start of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, \n",
    "                 learning_rate = 0.2, discount_factor = 1.0,\n",
    "                 exploration_rate = 0.5, exploration_decay_rate = 0.99,\n",
    "                 n_bins = 9, n_actions = 2, splits=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate # initial epsilon\n",
    "        self.exploration_decay_rate = exploration_decay_rate # decay factor for epsilon\n",
    "        self.n_bins = n_bins\n",
    "        self.n_actions = n_actions\n",
    "        self.splits = splits\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "        if self.splits is None: #CartPole default\n",
    "            self.splits = [\n",
    "                # Position\n",
    "                np.linspace(-2.4, 2.4, self.n_bins)[1:-1],\n",
    "                # Velocity\n",
    "                np.linspace(-3.5, 3.5, self.n_bins)[1:-1],\n",
    "                # Angle.\n",
    "                np.linspace(-0.5, 0.5, self.n_bins)[1:-1],\n",
    "                # Tip velocity\n",
    "                np.linspace(-2.0, 2.0, self.n_bins)[1:-1]\n",
    "            ]\n",
    "        \n",
    "        # Create Q-Table\n",
    "        num_states = (self.n_bins+1) ** len(self.splits)\n",
    "        self.q_table = np.zeros(shape=(num_states, self.n_actions))\n",
    "\n",
    "    # Turn the observation into integer state\n",
    "    def set_state(self, observation):\n",
    "        state = 0\n",
    "        for i, column in enumerate(observation):\n",
    "            state +=  np.digitize(x=column, bins=self.splits[i]) * ((self.n_bins + 1) ** i)\n",
    "        return state\n",
    "\n",
    "    # Initialize for each episode\n",
    "    def init_episode(self, observation):\n",
    "        # Gradually decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay_rate\n",
    "\n",
    "        # Decide initial action\n",
    "        self.state = self.set_state(observation)\n",
    "        return np.argmax(self.q_table[self.state])\n",
    "\n",
    "    # Select action and update\n",
    "    def act(self, observation, reward=None, done=None, mode='train'):\n",
    "        next_state = self.set_state(observation)\n",
    "        \n",
    "        if mode == 'test':\n",
    "            # Test mode \n",
    "            next_action = np.argmax(self.q_table[next_state])\n",
    "        else:\n",
    "            # Train mode by default\n",
    "            # Train by updating Q-Table based on current reward and 'last' action.\n",
    "            self.q_table[self.state, self.action] += self.learning_rate * \\\n",
    "                (reward + self.discount_factor * max(self.q_table[next_state, :]) - self.q_table[self.state, self.action])\n",
    "            # Exploration or exploitation\n",
    "            do_exploration = (1 - self.exploration_rate) < np.random.uniform(0, 1)\n",
    "            if do_exploration:\n",
    "                #  Exploration\n",
    "                next_action = np.random.randint(0, self.n_actions)\n",
    "            else:\n",
    "                # Exploitation\n",
    "                next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CartPole with the Q-Learning agent <a name='qlagent' />\n",
    "\n",
    "### Preparation: Initialize the Q-learning agent and training parameters\n",
    "\n",
    "Beginning from an empty Q-table, QLearningAgent tries to learn $Q(s_t, a_t)$ by Q-learning with $\\epsilon$-greedy strategy in 50 episodes. The result gif shows the agent gradually learns from trials and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "q_agent = QLearningAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "max_length = 200\n",
    "initial_reward = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Train the Q-learning agent\n",
    "\n",
    "Each time the learner cell is executed, the agent improves its policy.  To start over from scratch with the agent, go back and execute the cell that instantiates the agent with `q_agent = QLearningAgent()` \n",
    "\n",
    "The result statistics of average and maximum steps achieved during the episodes shows that over time the agent is able to improve its behavior.  We can see this in the visualization as the CartPole is able to stay verticle for longer periods of time after more training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:00<00:02, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 50.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the agent - execute this cell as many times as you wish\n",
    "# set the visualize_plt flag to True to see the cart in the notebook.  \n",
    "# note that this will run slower if visualized\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length, \n",
    "                init_reward=initial_reward, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 50 episodes: 11.0\n",
      "Average step count in 50 episodes: 20.96\n",
      "Maximum step count in 50 episodes: 92.0\n",
      "Q-table size:  20000\n",
      "Q-table nonzero count:  108\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABHxJREFUeJzt3NtNW0EUQFHfyE2kjqSM1AE1QR0pI6kjZTg/loKM8QPLmTvba0l8gGR0PszW6M7By2632wDQ82X0AADch8ADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNR29AB7/p0W4L3llhc7wQNECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNR29AAwyu/X53c/+/b0MmASuA8neIAogQeIEniAKIEHiBJ4gCiB52Ed25g5tlkDsxJ4gCiBB4gSeDjgMQ0VAg8QJfAAUQLPQ/PZM5QJPECUwMMRLlopEHiAKIEHiBJ4Hp6LVqoEHiBK4OEDLlqZncADRAk8QJTAA0QJPGxs0tAk8ABRAg8n2KRhZgIPECXwAFECD3suWqkReIAogYczXLQyK4EHiBJ4gCiBB4gSeHjDJg0lAg8QJfBw4Ngp3iYNMxJ4gCiBB4gSeLiQxzTMRuABogQeIErg4Qj78BQIPECUwMMVXLQyE4EHiBJ4gCiBhw+4aGV2Ag8QJfBwJRetzELgAaIEHiBK4AGiBB5OsEnDzAQeIErg4RNs0jADgQeIEniAKIEHiBJ4OMMmDbMSeB7asiwXfd3y2lO/A+5J4OGTfr08jR4BTtqOHgBm8vPPv6j/+Po6cBI4zwkeLvQ27se+h7UReLjA92endeYj8ABRAg8XOnzm7hk8a7fsdrvRM2w2m80qhuDx/M/1xZX8rTGXm96gq9iisSPMI/A+51q3HgpWEXgnG0ZxgqfMM3iAKIEHiBJ4gCiBB4gSeIAogQeIEniAqFXswcModtMpc4IHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4jajh5gbxk9AECNEzxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEDUX2apST/KrMN7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success and NOT exploring\n",
    "\n",
    "num_episodes = 5\n",
    "max_length = 200\n",
    "initial_reward = 1\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length,\n",
    "            init_reward=initial_reward, mode='test') # set mode to 'test' to avoid exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 5 episodes: 15.0\n",
      "Average step count in 5 episodes: 21.0\n",
      "Maximum step count in 5 episodes: 32.0\n",
      "Q-table size:  20000\n",
      "Q-table nonzero count:  108\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Example: MountainCar-v0 <a name='mtn' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is full of environments to try with varying difficulties and challenges.  Start by reviewing the information found in the [wiki](https://github.com/openai/gym/wiki/Table-of-environments).  Here's some information provided about the [MountainCar-v0 environment](https://github.com/openai/gym/wiki/MountainCar-v0):\n",
    "\n",
    "<img src=\"image/mtncar.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "Get an under powered car to the top of a hill (top = 0.5 position).\n",
    "\n",
    "#### Observation\n",
    "Type: Box(2)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Position | -1.2 | 0.6\n",
    "1 | Velocity | -0.07 | 0.07\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(3)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | push left\n",
    "1 | no push\n",
    "2 | push right\n",
    "\n",
    "#### Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "#### Starting state\n",
    "Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "#### Episode termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: MountainCar with the Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d30d6a414a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try mountain car with the Random agent (default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MountainCar-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-e06d43dba839>\u001b[0m in \u001b[0;36mlearner\u001b[0;34m(agent, env_id, episodes, max_length, init_reward, ignore_done, visualize_plt, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just update the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2261\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2263\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2264\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1475\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2605\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 593\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    839\u001b[0m         return self._make_image(\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m# Always convert to RGBA, even if only RGB input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb_to_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_rgb_to_rgba\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \"\"\"\n\u001b[1;32m    174\u001b[0m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try mountain car with the Random agent (default)\n",
    "steps = learner(env_id='MountainCar-v0', episodes=3, max_length = 200, init_reward=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: MountainCar with the Q-Learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the Q-Learning agent, we need to consider how the environment setup will affect the learning for our agent.  The MountainCar-v0 problem is a hard one because the agent can't really learn what actions are helpful until it actually climbs the hill successfully.  As long as that goal has not been met, all the episodes will have an accumulated reward of -1 for each timestep, or -200, since the number of steps allowed is capped.  Therefore, it's important that the car explores as many avenues as possible to solve the problem.  As a help for this notebook, we can extend the number of timesteps for the learner by ignoring the \"done\" signal and setting a higher number for the `max_length` parameter in learner. A special case provision has been included in the learner for this operation specifically for MountainCar-v0\n",
    "\n",
    "The Q-Learning agent also needs a way to discretize the observations for MountainCar, just as it did for the CartPole problem.  Although the agent itself will use the same basic iterative algorithm, we need to change the setup a bit.  This can be accomplished by setting up a different table of \"splits\".\n",
    "\n",
    "Whereas in CartPole, the split was:\n",
    "``` python\n",
    "\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-2.4, 2.4, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-3.5, 3.5, n_bins)[1:-1],\n",
    "    # Angle.\n",
    "    np.linspace(-0.5, 0.5, n_bins)[1:-1],\n",
    "    # Tip velocity\n",
    "    np.linspace(-2.0, 2.0, n_bins)[1:-1]\n",
    "    ]\n",
    " ```\n",
    " \n",
    "MountainCar only has two observation values:\n",
    "``` python\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-1.2, 0.6, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-0.07, 0.07, n_bins)[1:-1],\n",
    "    ]\n",
    "```\n",
    "In addition, the MountainCar problem has three actions whereas the CarPole only had two, so the agent will need to knw that . The Q-Learning agent defined previously has a provision for passing all the parameters it uses including the number of actions and splits.  Go ahead and create a split table now for the MountainCar Q-Learning agent and define the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define the number of bins `n_bins` and a list name `splits_mtncar` for use by the Q-Learning agent.\n",
    "# The n_bins used for CartPole was 9, but feel free to experiment with this number\n",
    "# n_bins = \n",
    "# splits_mtncar = \n",
    "\n",
    "\n",
    "# ANSWER\n",
    "n_bins = 20\n",
    "splits_mtncar = [\n",
    "    # Position\n",
    "    np.linspace(-1.2, 0.6, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-0.07, 0.07, n_bins)[1:-1],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate a QLearningAgent named q_agent_mtncar\n",
    "# you may want to tweak the hyper-parameters, such as the exploration rate, to increase\n",
    "#    the agent's chances of climbing the hill\n",
    "# q_agent_mtncar = \n",
    "\n",
    "\n",
    "# ANSWER\n",
    "q_agent_mtncar = QLearningAgent(learning_rate = 0.2, discount_factor = 0.9,\n",
    "                                exploration_rate = 0.8,\n",
    "                                n_actions = 3, n_bins = n_bins, splits = splits_mtncar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent long enough for it to achieve success within 500 steps\n",
    "# Feel free to modify any of these parameters\n",
    "num_episodes = 50\n",
    "max_length = 2000\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success\n",
    "num_episodes = 3\n",
    "max_length = 500\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=True, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!  \n",
    "Now you are on your way to trying other environment problems and RL algorithms.  Feel free to add additional cells here in order to try additional environments in OpenAI Gym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

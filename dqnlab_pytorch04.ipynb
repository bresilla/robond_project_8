{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN) Lab Notebook\n",
    "\n",
    "In this notebook, you will learn the basics applying the DQN algorithm to simple environments in OpenAI Gym.  By the end of the notebook, you will have a working DQN agent that can balance a CartPole as well as solve other simple control problems in alternate environments.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. <a href=#pix>Getting Started with Pixels to Actions</a>\n",
    "- <a href=#setup>Setup</a>\n",
    "- <a href=#dqn>DQN Algorithm</a>\n",
    "- <a href=#nn>Q-Network model</a>\n",
    "- <a href=#train>Training</a>\n",
    "\n",
    "## Requirements\n",
    "* OpenAI Gym[classic_control] 0.7.4 or higher \n",
    "* Python 3.5 or higher\n",
    "* pyTorch 0.4\n",
    "\n",
    "Portions of code and explanations in this notebook have been borrowed from the [PyTorch DQN Tutorial by Adam Paszke](https://github.com/apaszke)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Pixels to Actions<a name='pix' />\n",
    "In the Q-Learning Lab notebook for this course, you were able to train an agent to balance the OpenAI Gym CartPole using a provided Q-Learning algorithm.  The Q-Learning algorithm read in the environment observations and reward provided by the `gym` CartPole-v0 environment api.  The Q-Learning agent used a table to store all the updated values and simply looked up the best learned value to make decisions.  \n",
    "\n",
    "This works great when we have an environment that can tell us exactly what is going on.  The CartPole-v0 environment provides the exact position, velocity, and angles of the pole!  But what if all we had was a picture of what was happening in the game?  \n",
    "<br><br>So instead of an observation like this (position, velocity, angle, tip velocity):\n",
    "\n",
    "```\n",
    "observation: [ 0.00326999 -0.17222302  0.01642742  0.30067511]\n",
    "```\n",
    "\n",
    "we have an observation like this (the actual image):\n",
    "<img src=\"image/cartpole.png?\" width=\"450\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As we've seen in other deep learning problems, a neural network can be trained to \"learn\" distinguishing features from an image. With a Deep Q-Network (DQN), we can learn to solve the CartPole problem by looking at the scene.  Because of this, our results aren't directly comparable to results that use the simpler observation scheme - this is a much harder task. Training is slower, because all frames have to be rendered.\n",
    "\n",
    "The algorithm will focus on a patch of the image where the cart exists, and determine the state based on the difference between the current and previous patches. This will allow the agent to take the velocity of the pole into account from one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup packages, gpu, utilities<a name='setup' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named pip\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install packages in Udacity Workspaces\n",
    "# DO NOT EXECUTE OFFLINE\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*from the PyTorch tutorial:*<br>\n",
    "# 3. DQN algorithm<a name='dqn' />\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "\n",
    "$$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$$\n",
    "\n",
    "where $R_{t_0}$ is also known as the *return*. The discount,\n",
    "$\\gamma$, should be a constant between $0$ and $1$\n",
    "that ensures the sum converges. It makes rewards from the uncertain far\n",
    "future less important for our agent than the ones in the near future\n",
    "that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "$$ \\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "However, we don't know everything about the world, so we don't have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble\n",
    "$Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "$$ \\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))$$\n",
    "\n",
    "To minimise this error, we will use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate\n",
    "this over a batch of transitions, $B$, sampled from the replay\n",
    "memory:\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "\n",
    "$$   \\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "   \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Q-network model<a name='nn' />\n",
    "\n",
    "Our model will be a convolutional neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and\n",
    "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
    "network). In effect, the network is trying to predict the *quality* of\n",
    "taking each action given the current input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: DQN network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Replay Memory\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "-  ``Transition`` - a named tuple representing a single transition in\n",
    "   our environment\n",
    "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
    "   transitions observed recently. It also implements a ``.sample()``\n",
    "   method for selecting a random batch of transitions for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions: Input extraction\n",
    "\n",
    "The code below are utilities for extracting and processing rendered\n",
    "images from the environment. It uses the ``torchvision`` package, which\n",
    "makes it easy to compose image transforms. Once you run the cell it will\n",
    "display an example patch that it extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADWCAYAAADBwHkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFL9JREFUeJzt3X2wXHV9x/H3J/cmIYQACQmZQFIvYIRCB4IiDyO1yIONtApOHZW2Fhys2uJIWh4EnKnY2qlMedAZO1QRlIriA4pgikIIoRarQAIBQwIkYJDEmydMTBAIefj2j/O7cHbv3bt79/mefF4zZ+7+zjl7zmfP3vu9Z3+7e36KCMzMbPQb0+kAZmbWHC7oZmYF4YJuZlYQLuhmZgXhgm5mVhAu6GZmBeGCbm0n6TxJD3Q6RzeR1CcpJPV2OouNXi7oBSNptaSXJb2Ym77U6VydJukUSWtauP0rJd3Squ2b1cJnA8X07oi4t9MhRhtJvRGxs9M5WqHIj81e5zP0PYik6yV9P9e+StJCZSZLmi9po6TN6fbM3Lr3S/qcpP9LZ/0/knSApG9K2irpYUl9ufVD0iclPStpk6R/lzTk75ukIyQtkPRbSU9Jev8wj2E/STdK6pe0NmXqqfL4JgI/Bg7KvWo5KJ1V3ybpFklbgfMkHS/p55K2pH18SdK43DaPymVdL+kKSXOBK4APpG0/VkPWHklXp2PzLPBnVZ67T6VtbEvH6LTcdq6Q9ExatkTSrNxzcIGklcDKasda0viU6dfpsf2npAlp2SmS1ki6SNKG9Jg+PFxm64CI8FSgCVgNnF5h2d7A08B5wB8Dm4CZadkBwF+kdSYB3wN+mLvv/cAq4DBgP2B52tbpZK/0/gv4Wm79ABYBU4A/SOt+JC07D3gg3Z4IPA98OG3n2JTryAqP4Xbgy+l+BwIPAR+r4fGdAqwp29aVwA7gbLKTmwnAW4ATU5Y+YAUwL60/CegHLgL2Su0Tctu6ZQRZPw48CcxKx2hROma9Qzzmw9MxOii1+4DD0u1LgF+mdQQcAxyQew4WpO1PqHasgeuAO9P6k4AfAf+WO347gX8GxgJnAi8Bkzv9O+8p97vS6QCemvyEZgX9RWBLbvrb3PITgN8CzwHnDLOdOcDmXPt+4NO59jXAj3PtdwNLc+0A5ubafw8sTLfP4/WC/gHgf8v2/WXgM0Nkmg5sBybk5p0DLKr2+Khc0H9a5XjOA27P7evRCutdSa6gV8sK3Ad8PLfsnVQu6G8ENpD98xxbtuwp4KwKmQI4NdeueKzJ/hn8nvSPIi07CfhV7vi9nM+XMp3Y6d95T69P7kMvprOjQh96RDyYXuIfCHx3YL6kvcnO0OYCk9PsSZJ6ImJXaq/PberlIdr7lO3u+dzt54CDhoj0BuAESVty83qBb1RYdyzQL2lg3pj8fio9vmHkMyLpTcC1wHFkZ/y9wJK0eBbwTA3brCXrQQw+PkOKiFWS5pH90zhK0t3AP0bEb2rIlN/HcMd6GtnjXZLLK6Ant+4LUdoP/xKDn3PrIPeh72EkXQCMB34DXJpbdBHZy/YTImJf4O0Dd2lgd7Nyt/8g7bPc88D/RMT+uWmfiPi7CutuB6bm1t03Io4aWGGYx1fpsqLl868n6wqZnY7DFbx+DJ4HDq1xO9Wy9jP4+FQUEd+KiJPJinIAV+X2c9hwdy3LVOlYbyL7p3xUbtl+EeGCPYq4oO9B0tnn54C/Bj4EXCppTlo8iewPeoukKWQvwxt1SXqzdRZwIfCdIdaZD7xJ0ockjU3TWyX9YfmKEdEP3ANcI2lfSWMkHSbpT2p4fOuBAyTtVyXzJGAr8KKkI4D8P5b5wAxJ89IbiJMknZDbft/AG7/VspK9evikpJmSJgOXVQok6XBJp0oaD7xC9jztTou/CvyLpNnKHC3pgAqbqnisI2I3cANwnaQD034PlvSnVY6XdREX9GL6kUo/h367si+s3AJcFRGPRcRKsrPPb6RC8QWyN842Ab8AftKEHHeQdVcsBf4buLF8hYjYRtZ//EGys+p1ZGef4yts82+AcWRvym4GbiMrssM+voh4ErgVeDZ9gmWo7h+Ai4G/BLaRFbjX/gmlrGeQvV+wjuyTI+9Ii7+Xfr4g6ZHhsqZlNwB3A48BjwA/qJCHdCw+T/bcrCPrTro8LbuW7J/DPWT/iG4kex4HqeFYf4rsje9fpE/93Ev2qs1GCUV4gAtrPklB1m2xqtNZzPYUPkM3MysIF3Qzs4Jwl4uZWUE0dIYuaW76+vAqSRXfpTczs9ar+ww9XZPiabJ3/dcAD5N9M295pftMnTo1+vr66tqfmdmeasmSJZsiYlq19Rr5pujxwKqIeBZA0reBs8g+ojWkvr4+Fi9e3MAuzcz2PJIqfpM4r5Eul4Mp/VrxmjSvPMhHJS2WtHjjxo0N7M7MzIbT8k+5RMRXIuK4iDhu2rSqrxjMzKxOjRT0tZRei2JmmmdmZh3QSEF/GJgt6RBlAwB8kOxaymZm1gF1vykaETslfYLsehQ9wE0R8UTTkpmZ2Yg0dD30iLgLuKtJWczMrAEe4ML2TGXfv9i9a8egVcb0jhs0z6yb+VouZmYF4YJuZlYQLuhmZgXhgm5mVhB+U9QKaderL5e0V99/c0n7lS3rStpTDz9p0DamH+PhNG108Rm6mVlBuKCbmRWEC7qZWUG4D90KKXbvKmlvW7uipL19a+mlnKcc+paWZzJrNZ+hm5kVhAu6mVlBNNTlImk1sA3YBeyMiOOaEcrMzEauGX3o74iITU3YjlnLqKd32DZSG9OYtYa7XMzMCqLRgh7APZKWSProUCt4kGgzs/ZotKCfHBFvBt4FXCDp7eUreJBoM7P2aKigR8Ta9HMDcDtwfDNCmTVflE1lS3fvGjSZjTZ1F3RJEyVNGrgNvBNY1qxgZmY2Mo18ymU6cLuyTwf0At+KiJ80JZWZmY1Y3QU9Ip4FjmliFjMza4Cv5WKF1DNuQkl7/KQDS9rbt5Z+deL3G59reSazVvPn0M3MCsIF3cysIFzQzcwKwgXdzKwg/KaoFZLG9JS0x4zba9j1/UUiKwKfoZuZFYQLuplZQbigm5kVhPvQbc8Qgy/IZVY0PkM3MysIF3Qzs4KoWtAl3SRpg6RluXlTJC2QtDL9nNzamGZmVk0tZ+hfB+aWzbsMWBgRs4GFqW1mZh1UtaBHxE+B35bNPgu4Od2+GTi7ybnMzGyE6u1Dnx4R/en2OrLBLobkQaLNzNqj4TdFI2LoQRpfX+5Bos3M2qDegr5e0gyA9HND8yKZmVk96i3odwLnptvnAnc0J46ZmdWrlo8t3gr8HDhc0hpJ5wOfB86QtBI4PbXNzKyDqn71PyLOqbDotCZnMTOzBvibomZmBeGCbmZWEC7oZmYF4YJuZlYQLuhmZgXhgm5mVhAu6GZmBeGCbmZWEC7oZmYF4YJuZlYQLuhmZgXhgm5mVhD1DhJ9paS1kpam6czWxjQzs2rqHSQa4LqImJOmu5oby6zJIkqnctLgyWyUqXeQaDMz6zKN9KF/QtLjqUtmcqWVPEi0mVl71FvQrwcOA+YA/cA1lVb0INFmZu1RdcSioUTE+oHbkm4A5jctkVkL9IyfOOzy3a++Mmhe7N5V0taYnqZmMmu2us7QJc3INd8LLKu0rpmZtUfVM/Q0SPQpwFRJa4DPAKdImgMEsBr4WAszmplZDeodJPrGFmQxM7MG1NWHbjba7D11Zkn7hadLl2/ftmHQfXa9+nJJu3evfZqey6yZ/NV/M7OCcEE3MysIF3Qzs4JwQTczKwi/KWp7hqEuyFXCF+Oy0c9n6GZmBeGCbmZWEC7oZmYF4YJuZlYQLuhmZgXhgm5mVhC1DBI9S9IiScslPSHpwjR/iqQFklamnxVHLTIzs9ar5Qx9J3BRRBwJnAhcIOlI4DJgYUTMBhamtpmZdUgtg0T3R8Qj6fY2YAVwMHAWcHNa7Wbg7FaFNDOz6kbUhy6pDzgWeBCYHhH9adE6YHqF+3iQaDOzNqi5oEvaB/g+MC8ituaXRUSQjV40iAeJNjNrj5oKuqSxZMX8mxHxgzR7/cDYounn4BECzMysbWr5lIvIhpxbERHX5hbdCZybbp8L3NH8eGZmVqtarrb4NuBDwC8lLU3zrgA+D3xX0vnAc8D7WxPRzMxqUcsg0Q9Q+dqipzU3jpmZ1cvfFDUzKwgXdDOzgnBBNzMrCBd0M7OCcEE3MysIF3Qzs4JwQTczKwgXdDOzgnBBNzMrCBd0M7OCcEE3MysIF3Qzs4JoZJDoKyWtlbQ0TWe2Pq6ZmVVSy+VzBwaJfkTSJGCJpAVp2XURcXXr4pk1R8TuKmsMcUFRVbrIqFl3quXyuf1Af7q9TdLAINFmZtZFGhkkGuATkh6XdJOkyRXu40GizczaoJFBoq8HDgPmkJ3BXzPU/TxItJlZe9TShz7kINERsT63/AZgfksSmjXB+H2nlrQ1pqekvXvHK4Pus/Ol35W0e8dPbH4wsyaqe5BoSTNyq70XWNb8eGZmVqtGBok+R9IcIIDVwMdaktDMzGrSyCDRdzU/jpmZ1aumPnSz0W78pOH70HfteHnQfXaU9aHvNfmg5gczayJ/9d/MrCBc0M3MCsIF3cysIFzQzcwKwm+K2h7BF+eyPYHP0M3MCsIF3cysIFzQzcwKwgXdzKwgXNDNzArCBd3MrCBquXzuXpIekvRYGiT6s2n+IZIelLRK0nckjWt9XDMzq6SWM/TtwKkRcQzZ6ERzJZ0IXEU2SPQbgc3A+a2LadaY3t7ekklE1an8PmbdrmpBj8yLqTk2TQGcCtyW5t8MnN2ShGZmVpOa+tAl9aTBLTYAC4BngC0RsTOtsgY4uMJ9PUi0mVkb1FTQI2JXRMwBZgLHA0fUugMPEm1m1h4j6hiMiC2SFgEnAftL6k1n6TOBta0IaHueRx99tKR98cUXN7zN2dP3Kml/5JRDq97nH+ZdWNJeuX7wQNIjdfXVV5e0jz322Ia3aTaglk+5TJO0f7o9ATgDWAEsAt6XVjsXuKNVIc3MrLpaztBnADdL6iH7B/DdiJgvaTnwbUmfAx4FbmxhTjMzq6KWQaIfBwa9LoyIZ8n6083MrAv4w7XWdV544YWS9n333dfwNte+oa+kfcTRl5a0g9JBowHu/dmHS9rP/HpVwznKH5tZM/mr/2ZmBeGCbmZWEC7oZmYF4YJuZlYQflPUuk4rLoTVM25SSXt3z5SS9qs7Bw8IPWbspEHzGuWLfFkr+QzdzKwgXNDNzArCBd3MrCDa2qG3Y8cO+vv727lLG4U2bdrU9G3+bsvqkvbP772kpL189eB9ru9f3vQc5Y/Nfw/WTD5DNzMrCBd0M7OCaGSQ6K9L+pWkpWma0/q4ZmZWSS196AODRL8oaSzwgKQfp2WXRMRtw9y3xM6dO/EwdFbNli1bmr7NtRu3lbRvu+fupu+jFuWPzX8P1ky1XD43gKEGiTYzsy5S1yDREfFgWvSvkh6XdJ2k8RXu+9og0Zs3b25SbDMzK1fXINGS/gi4nGyw6LcCU4BPVbjva4NET548uUmxzcysXL2DRM+NiIHRbrdL+hpQdSTfCRMmcPTRR9cR0/YkRX4lN3v27JK2/x6smeodJPpJSTPSPAFnA8taGdTMzIbXyCDR90maBghYCny8hTnNzKyKRgaJPrUliczMrC6+OLN1nR07dnQ6QssU+bFZ5/mr/2ZmBeGCbmZWEC7oZmYF4YJuZlYQflPUus7UqVNL2qeffnqHkjRf+WMzayafoZuZFYQLuplZQbigm5kVhPvQrevMmVM6+NWCBQs6lMRsdPEZuplZQbigm5kVhAu6mVlBKBsytE07kzYCzwFTgU1t23H9nLO5RkPO0ZARnLPZuj3nGyJiWrWV2lrQX9uptDgijmv7jkfIOZtrNOQcDRnBOZtttOSsxl0uZmYF4YJuZlYQnSroX+nQfkfKOZtrNOQcDRnBOZtttOQcVkf60M3MrPnc5WJmVhAu6GZmBdH2gi5prqSnJK2SdFm791+JpJskbZC0LDdviqQFklamn5M7nHGWpEWSlkt6QtKFXZpzL0kPSXos5fxsmn+IpAfTc/8dSeM6mXOApB5Jj0qan9pdl1PSakm/lLRU0uI0r6ue95Rpf0m3SXpS0gpJJ3VTTkmHp2M4MG2VNK+bMjairQVdUg/wH8C7gCOBcyQd2c4Mw/g6MLds3mXAwoiYDSxM7U7aCVwUEUcCJwIXpOPXbTm3A6dGxDHAHGCupBOBq4DrIuKNwGbg/A5mzLsQWJFrd2vOd0TEnNznpbvteQf4IvCTiDgCOIbsuHZNzoh4Kh3DOcBbgJeA27spY0Miom0TcBJwd659OXB5OzNUydcHLMu1nwJmpNszgKc6nbEs7x3AGd2cE9gbeAQ4geybeL1D/S50MN9Msj/gU4H5gLo052pgatm8rnregf2AX5E+bNGtOXO53gn8rJszjnRqd5fLwcDzufaaNK9bTY+I/nR7HTC9k2HyJPUBxwIP0oU5UzfGUmADsAB4BtgSETvTKt3y3H8BuBTYndoH0J05A7hH0hJJH03zuu15PwTYCHwtdWF9VdJEui/ngA8Ct6bb3ZpxRPymaI0i+9fdFZ/xlLQP8H1gXkRszS/rlpwRsSuyl7UzgeOBIzocaRBJfw5siIglnc5Sg5Mj4s1k3ZUXSHp7fmGXPO+9wJuB6yPiWOD3lHVddElO0vsi7wG+V76sWzLWo90FfS0wK9eemeZ1q/WSZgCknxs6nAdJY8mK+Tcj4gdpdtflHBARW4BFZF0X+0saGFSlG577twHvkbQa+DZZt8sX6b6cRMTa9HMDWZ/v8XTf874GWBMRD6b2bWQFvttyQvaP8ZGIWJ/a3ZhxxNpd0B8GZqdPEYwje8lzZ5szjMSdwLnp9rlkfdYdI0nAjcCKiLg2t6jbck6TtH+6PYGsn38FWWF/X1qt4zkj4vKImBkRfWS/i/dFxF/RZTklTZQ0aeA2Wd/vMrrseY+IdcDzkg5Ps04DltNlOZNzeL27Bboz48h14I2IM4GnyfpUP93pNxFyuW4F+oEdZGca55P1py4EVgL3AlM6nPFkspeCjwNL03RmF+Y8Gng05VwG/FOafyjwELCK7KXu+E4/77nMpwDzuzFnyvNYmp4Y+Lvptuc9ZZoDLE7P/Q+Byd2WE5gIvADsl5vXVRnrnfzVfzOzgvCbomZmBeGCbmZWEC7oZmYF4YJuZlYQLuhmZgXhgm5mVhAu6GZmBfH/UEU4UW9i4JcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training<a name='train' />\n",
    "\n",
    "\n",
    "## Hyperparameters and utilities\n",
    "\n",
    "The next two cells set the hyperparameters, instantiate our model and its optimizer, and define two\n",
    "utilities:\n",
    "\n",
    "-  ``select_action`` - will select an action accordingly to an epsilon\n",
    "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
    "   the action, and sometimes we'll just sample one uniformly. The\n",
    "   probability of choosing a random action will start at ``EPS_START``\n",
    "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
    "   controls the rate of the decay.\n",
    "-  ``plot_durations`` - a helper for plotting the durations of episodes,\n",
    "   along with an average over the last 100 episodes (the measure used in\n",
    "   the official evaluations). The plot will be underneath the cell\n",
    "   containing the main training loop, and will update after every\n",
    "   episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: action selection and plot  durations utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "     \n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: single step optimization\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an ``optimize_model`` function that performs a\n",
    "single step of the optimization. It first samples a batch, concatenates\n",
    "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
    "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
    "state.\n",
    "\n",
    "We also use a target network to compute $V(s_{t+1}) for added stability. The target network has its weights kept frozen most of the time, but is updated with the policy network’s weights every so often. This is usually a set number of steps but we shall use episodes for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: main training loop\n",
    "\n",
    "Below, you can find the main training loop wrapped in the `dqn_training` definition. At the beginning we reset\n",
    "the environment and initialize the ``state`` Tensor. Then, we sample\n",
    "an action, execute it, observe the next screen and the reward (always\n",
    "1), and optimize our model once. When the episode ends (our model\n",
    "fails), we restart the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the main training loop in a definition\n",
    "\n",
    "def dqn_training(num_episodes, visualize_plt=False, max_steps=500):\n",
    "    \"\"\"\n",
    "    num_episodes: int \n",
    "        number of episodes\n",
    "    visualize_plt: bool\n",
    "        if true, display the cartpole action in the notebook\n",
    "        if false (default), display the episodes x durations graph\n",
    "    \"\"\"\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        if visualize_plt:\n",
    "            img = plt.imshow(env.render(mode='rgb_array')) # only call this once, only for jupyter\n",
    "\n",
    "        last_screen = get_screen()\n",
    "        current_screen = get_screen()\n",
    "        state = current_screen - last_screen\n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            action = select_action(state)\n",
    "            _, reward, done, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            current_screen = get_screen()\n",
    "            if visualize_plt:\n",
    "                img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "                plt.axis('off')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model()\n",
    "            if done or t>max_steps:\n",
    "                episode_durations.append(t + 1)\n",
    "                if visualize_plt:\n",
    "                    print(\"Duration = {}\".format(t))\n",
    "                else:\n",
    "                    plot_durations()\n",
    "                break\n",
    "        # Update the target network\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print('Complete')\n",
    "    env.render(close=True)\n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Train the model\n",
    "Below, `num_episodes` is set small. You can run this cell over an over to increase training.  The durations graph shows the duration of the cartpole balance for each episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named '_tkinter', please install the python3-tk package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.5/tkinter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named '_tkinter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ef8c4c88e0d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# note that the duplicate %matplotlib backend call is needed to avoid duplicate graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdqn_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2129\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2131\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-107>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3049\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3051\u001b[0;31m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3052\u001b[0m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# This must be imported last in the matplotlib series, after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0m_backend_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_figure_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylab_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/__init__.py\u001b[0m in \u001b[0;36mpylab_setup\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# imports. 0 means only perform absolute imports.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     backend_mod = __import__(backend_name, globals(), locals(),\n\u001b[0;32m---> 62\u001b[0;31m                              [backend_name], 0)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Things we pull in from all backends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_tkagg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtkagg\u001b[0m  \u001b[0;31m# Paint image to Tk photo blitter extension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend_agg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m from ._backend_tk import (\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/backends/tkagg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtkinter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, tp)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Invokes __set__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36m_import_module\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_import_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m\"\"\"Import module, returning the module after the last dot.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/tkinter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', please install the python3-tk package'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mTclError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTclError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named '_tkinter', please install the python3-tk package"
     ]
    }
   ],
   "source": [
    "# note that the duplicate %matplotlib backend call is needed to avoid duplicate graphs\n",
    "%matplotlib\n",
    "%matplotlib    \n",
    "num_episodes=5\n",
    "dqn_training(num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: CartPole visualization\n",
    "Execute the following cell at any point to watch the trained DQN agent control the Cartpole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_episodes=1\n",
    "dqn_training(num_episodes, visualize_plt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
